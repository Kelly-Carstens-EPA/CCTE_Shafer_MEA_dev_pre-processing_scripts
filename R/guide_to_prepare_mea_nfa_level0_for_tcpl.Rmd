---
title: "Guide to Prepare Data from Network Formation Assay Experiments for Dose-Response Modeling with the ToxCast Pipeline"
author: "Amy Carpenter"
last updated: "July 5, 2023"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = FALSE}
library(data.table)
library(stringi)
print(sessionInfo())
```

# Known issues with the pre-processing workflow

* Would be better to read the cytotoxicity data from the raw files directly... ugh. This is semi-critical but rare. Maybe see what you do for the acute? Theoretically this coudl really save some steps...
* The source files
* Background: All experimental and meta data sources files used to pre-process data for the ToxCast Pipeline will ultimately be saved in Clowder (to Amy's knowledge). With the current pre-processing approach, the file paths to the source files used are recorded in the files_log.txt. However, these source files could be modified after the pre-processing is completed. Therefore, theoretically the source files should be copied to a stable location before running the pre-processing scripts. However, this seems like it could result in a lot of data copying, especially if the files will need to be copied again to the ToxCast_data drive. Going forward, work with the ToxCast data team to determine the best path, and check for any modifications to the source files that occurred after pre-processing before sharing with the ToxCast team.
* The 'srcf' columns:
* Background: The mc0 and sc0 files that are loaded into the ToxCast pipeline are expected to include the column 'srcf' (read "source file"). This column should contain the name of the file that was used to generate the data
* However, for the NFA, usually multiple source files are used to generate the numeric and meta data for a single data row
* Thus, a solution is to package the source files into .zip files (e.g., 1 for each plate for the MEA endpoints). These .zip files would become the new 'srcf'. This solution was proposed by Jason Brown to address a similar issue for the HCI data.

# Completed

* Quickly search for old notes of things to do for NFA / general pre-processing. Triage those somewhere where I will seem them. Goal is to priortize (noting only have so much time left!)
* Making this an RMD -> going to do it!

# Next steps

*** Monday = minimally viable product!! *********
COB - have something that will work, with known issues / bugs / to-d articulated at the top, in such a way that Man/Kelly could interpret

* Go through the run_me, make it good, and make this guide actually follow that! (time to go line by line & make it work!!)
* Make it a working RMD as you go!
* Review recent updates to the HCI data pre-processing. Note any to integrate here or in run_me.
* Cross check template with most recent NFA run_me that has been ran
* To update in scripts as you go
* Confirm that package load order is okay
* well quality notes - updated name & format of wllq notes tables (see how doing it for the HCI!)
* make sure include functionality to merge wllq notes / wllq within and across tables
* check back with the notes template to confirm that instructions align with function
* See "to do now" in shafer lab pre-pro, documents, NFA pre-pro to do Now section for any funtionality notes to address now.
* See neat updates from e.g., TSCA2019 for handling larger datasets for documenting wllq...? (could I triage to later if there is more time?)
* Would this run_me work for sc too? Give some hints of what to do for M if she encounters that.
* Update/revisit/confirm the detaisl on parameter calculations? at least save that somewhere
* Add note for chgv_paraemers.R - this is loaded in create_ont_csv. So that's where you coudl modify!

**Checky-checky stuff**:

* Once you have verified that the wllq tables load okay, delete that deprecated page from the OneNote
* Re-name the GitHub repo from 'test' to final version, when that is ready (and if migrates?)
* Should the pre-pro project folder really be located here (`L:\Lab\NHEERL_MEA\CCTE_Shafer pre-process for TCPL\project_repos\CCTE_Shafer_MEA_dev_pre-processing_projects`)? Or will this be the archive/deprecated version?
* Be sure there is a script named `run_me_teamplte.R`, else re-name here
* See if can integrate 'run_me_wide' into the same script. See notes below for what I previously thought was relevant:

i.e., if you aren’t preparing data for tcpl, but just for e.g. Tim
Overlap with above process:
-	Packages to install mostly the same
-	Create a copy of the template folder, with the wllq to update thing
-	Notes template – I think can use the exact same process 
-	Run_me -> just have to select the “wide” version

If don’t have cytotox data, just select ‘n’ at Cytotoxicity data collection step.
Console will say ‘user elected to stop’. This is fine, you can move on.

# Would loooooove to do if time to make future debugging easy

* Document all I've learned about create ont burst data, so people know what is going on
* Make that funciton way more streamlined!!

* clean up creat ont csv

# Info that someoneo outside EPA would need

* how maestroexperimentlog files should be formatted - maybe include an example?

# Abbreviations

```{r echo = FALSE}
abb.tb <- rbind(
  data.table('NFA','Network Formation Assay'),
  data.table('MCL or Master Chemical List','MaestroExperimentLog file'),
  data.table('wllq','well quality')
)
names(abb.tb) <- c('Abbrev','Defn')
abb.tb[order(Abbrev)]
```

# Notes

* This document and the run_me_template.Rmd are meant to be living documents - update them as needed as the process evolves!
* Some things will be shfaer lab specific... I"M thinking of jsut givign this out to the public as-is, and people can adapt as needed (bc there are so many notes here that the average user would need, e.g., to ignore the 'no network spikes found' warning)

# R things I'd suggest Manasvinee learn (That I haven't already sent her)

(send her this as need arises & to break her in a bit)

* .Rproj, default settings re saving history!!
* shortcut to restart R, it's recommened
* `%in%`, match()
* stringi, regex (sent her some, but not stringi specifically)
* lists, maybe? She is already familar with vectors
* other data types (vectors vs list)
* functions - I already sent her some stuff, but maybe more?
* sapply, lapply
* data.table basics (e.g., :=, NULL, etc. I already sent her the vignette on data.table, but it didn't include := !)
* peak into the wild wild west of editing functions - how to open up the standardized scripts, what those look like...


# Potentially helpful RMD tips

* For any code chunk that you have completed and that you don't want to have to run again, you can set R chunk options to `eval = FALSE`.
* When you re-open the run_me after restarting RStudio (aka Posit) you resume from where you left off by running all code chunk above where you left off. Go to the current code chunk, and click the triangle pointing down or type Ctrl + alt + shift + P.
* If is runnign really slow from the .Rproj on the L drive, you can create a .Rproj on your local machine and open the .Rmd from there

# Potentially helpful NFA-specific notes

* Background on sjemea, then meadq made on top of that (links). Then i made some functions that replace some of the ones from meadq. So... there are 3 places where functions may reside where may have to debug

# Notes to Kathleen

## maestro experiment log
* Please keep the phrase "MaestroExperimentLog" in these files
* spike list files are matched to the maestro experiment log by the present of the culturedate_platenumber[_ ]. OKay?

# Overview of the process

This document outlines the process of transforming the recordings from a Microelectrode Array Network Formation Assay into an “mc0” file which can be analyzed with the ToxCast Pipeline R package. Intermediate outputs of the pre-processing steps include:

* A csv file containing well-level activity metrics for 17 endpoints across each DIV 
*	A csv file containing area-under-the-curve values of the well-level activity metrics for 17 endpoints


# Setting up the pre-processing project folder

* Define the project name for the project (usually the name of the compound set followed by the year the experiments were started, e.g. “PFAS2018”). Don’t use any spaces.
* Create a copy of the Template folder in this directory: `L:\Lab\NHEERL_MEA\CCTE_Shafer pre-process for TCPL\project_repos\CCTE_Shafer_MEA_dev_pre-processing_projects`. Rename the copy of the folder with the project name.

# Prepare notebook section

* Open the pre-processing notebook for the NFA: `L:\Lab\NHEERL_MEA\CCTE_Shafer pre-process for TCPL\OneNote Notebooks\Pre-processing_MEA_NFA_for_TCPL` 
* Make a copy of the `Notes Template` section and rename it with the project name
* Use this notebook section to record to-do items, take notes, and save email records as you pre-process the data. Review the suggested usage of this notebook in the "Overview" page. 

# Check out the project folder

Familiarize yourself with the project folder. Get a sense of where all of the culture folders are located, how many there are, which are single- and which are multiple-concentration screens,  etc.

If this data has already been published, determine which cultures were used for the analysis, and which were excluded (because of quality concerns). See "Experimental Summary of Data" file if present.

# Review lab notebook for well quality notes

* Read through the lab notebook for well quality notes (usually you can just scan for any hand-written notes)
* Determine if any cultures, plates, or wells should be excluded because of a misdose, contamination, bad culture, etc.
* Also note if any compounds are dissolved in anything other than DMSO
* Enter any well quality notes into the appropriate table, as described below

# Adjust well quality or add well quality notes

**Background**: 
Every level 0 table processed with the ToxCast pipeline must contain a binary 'wllq' column (read "well quality"), where 1 indicates that the data is usable and 0 that the data should not be used. In these pre-processing scripts, all data is assumed to have wllq = 1 unless otherwise indicated in the well quality tables (as described below). The well quality tables and pre-processing scripts also include an option to add a well quality note, regardless of whether the wllq is 1 or 0. These well quality notes are not included in the final level 0 table for the ToxCast pipeline; rather, these notes can be used for internal record-keeping and follow-up analyses.

There are 2 ways to set the wllq to 0 or add a well quality note (described below). Either option can be used; choose whichever option is easiest for each wllq situation. 

Note that the wllq notes entered in the first table ("[project_name]_well_quality_assignments_by_well.csv") will be applied from within the function parameter_values_to_AUC(). The wllq notes in the second table ("[project_name]_well_quality_assignments_by_compound_cndx_culture_date.csv") will be applied within the run_me RMD AFTER the treatment names and concentrations have been verified by you.


## Update the wllq when the well ID is known

For wells where you know the specific plate, well and (optionally) DIV that are affected:
Enter wells that should be excluded from the analysis in the excel file "[project_name]_well_quality_assignments_by_well.csv"

For each well, fill out the columns:

```{r echo = FALSE}
wllq.by.well.desc <- data.table(
  'Column' = c('affected_assays','date','Plate.SN','well','DIV','wllq','wllq_notes','wllq_ref'), 
  'Description' = c("Comma-separated list of which assays the well-quality affects, including mea, CTB (Cell Titer Blue), or LDH. (Note that the CTB assay may also be referred to as Alamar Blue)",
                    "Culture date (YYYYMMDD)",
                    "Plate Serial Number. Should begin with 'MW'","Affected well. Should begin with a letter (for the row) and end with a number. If all of the wells on a given plate are affected, enter 'all'.",
                    "Day in vitro. If all DIV are affected, enter 'all'. (If any DIV other than the standard expected DIV (5, 7, 9, and 12) are affected, create a separate row in the well quality table for each DIV). If the affected assay is a cytotoxicity assay (LDH or CTB), this column will be ignored.",
                    "Well quality (1 = usable, 0 = not usable)","Well quality note ",
                    "Well quality reference (i.e., the source from which you learned about the well quality note)"),
  'Examples' = c("For only the CellTiter Blue assay: CTB; For all assays: nfa,CTB,LDH",
                 "20201109",
                 "MW71-7905",
                 "A4",
                 "12",
                 "1; 0",
                 "Contamination; Misdosed; Recording too short; Temperature lower than usual",
                 "Lab notebook; Readme note")
)
wllq.by.well.desc

# max(nchar(wllq.by.well.desc$Column)) # 15
# wllq.by.well.desc[, num_spaces_to_add := 15 - nchar(Column)]
# wllq.by.well.desc[, col_for_tb := paste0('| ',Column,rep(' ',num_spaces_to_add),' |')]
# wllq.by.well.desc$col_for_tb <- unlist(lapply(1:nrow(wllq.by.well.desc), 
#                                               function(i) paste0('| ',wllq.by.well.desc[i,Column],
#                                                                  paste0(rep(' ',wllq.by.well.desc[i,num_spaces_to_add]),collapse = ''),
#                                                                  ' |')))
# cat(wllq.by.well.desc$col_for_tb, sep = '\n|-----------------|\n')
# 
# # Add correct number of spaces to the tail
# # gsub to get the split
# 
# # Say 65 is the max length
# wllq.by.well.desc[, desc_nchar := nchar(Description)]
# wllq.by.well.desc[, desc_final_row_char := desc_nchar%%65]
# wllq.by.well.desc[, desc_num_spaces_to_add := 65 - desc_final_row_char]
# wllq.by.well.desc$desc_for_tb <- unlist(lapply(1:nrow(wllq.by.well.desc), 
#                                               function(i) paste0(' ',wllq.by.well.desc[i,Description],
#                                                                  paste0(rep(' ',wllq.by.well.desc[i,desc_num_spaces_to_add]),collapse = ''),
#                                                                  ' |')))
# cat(wllq.by.well.desc$desc_for_tb, sep = paste0('\n',paste0(rep('-',67),collapse = ''),'|\n'))
# 
# # Now just add line breaks
# # wllq.by.well.desc[, desc_for_tb := sub('^.{66}','')]
# 
# # Maybe it's good enough
# 
# wllq.by.well.desc[, tb_row := paste0(col_for_tb, ' ',desc_for_tb)]
# cat(wllq.by.well.desc$tb_row, sep = paste0('\n|-----------------|',paste0(rep('-',68),collapse = ''),'|\n'))
# 
# 
# # Prep the example column
# # Just keep this simple, will manually update
# wllq.by.well.desc[, ex_final_string := stri_extract(Examples,regex = '[^\n]*$')]
# wllq.by.well.desc[, max(nchar(ex_final_string))] # 28
# wllq.by.well.desc[, ex_for_tb := paste0(' ',gsub('\n','\n ',Examples),' |')]
# 
# cat(wllq.by.well.desc$ex_for_tb, sep = paste0('\n',paste0(rep('-',30),collapse = ''),'|\n'))
# 
# wllq.by.well.desc[, tb_row := paste0(tb_row, ' ',ex_for_tb)]
# cat(wllq.by.well.desc$tb_row, sep = paste0('\n|-----------------|',
#                                            paste0(rep('-',68),collapse = ''),'|',
#                                            paste0(rep('-',30),collapse = ''),'|\n'))
# 
# 
# wllq.by.well.desc$Examples

```


## Update the wllq when the treatment is known

For wells where you know the treatment, culture date, and (optionally) concentration index that are affected:
Enter info in the table "[project_name]_well_quality_assignments_by_compound_cndx_culture_date.csv"

**Note that the well quality notes in this table will be added after the data has been transformed to the long format and the treatment, concentrations meta-data has been verified in the run_me script. Therefore, these wllq notes will not be present in the AUC, cytotox, and parameters_by_DIV intermediate output files. 

For each well, fill out the columns:

```{r echo = FALSE}
wllq.by.well.trt <- data.table(
  'Column' = c("affected_assays","compound_name","cndx","culture_date","wllq","wllq_notes","wllq_ref"),
  'Description' = c("Comma-separated list of which assays the well-quality affects (nfa, CTB (Cell Titer Blue), or LDH) . (Note that the CTB assay may also be referred to as Alamar Blue)","Name of affected compound (treatment), as it appears in the MaestroExperimentLog file","Concentration index tested for the given treatment, with 1 = lowest concentration tested and 7 = highest concentration. Note that this should be the concentration index ON A GIVEN PLATE (rather than all concentrations tested for the treatment in the project). If all concentrations tested are affected, enter 'all.'","Culture date (YYYYMMDD)","Well quality (1 = usable, 0 = not usable)","Well quality note ","Well quality reference (i.e., the source from which you learned about the well quality note)"),
  'Examples' = c("For only the CellTiter Blue assay: CTB; For all assays: nfa,CTB,LDH",
                 "7126 C7; DMSO","1","20201109","1; 0",
                 "Precipitate observed, actual concentration is dubious; Cell debris",
                 "Lab notebook; Readme note"))
wllq.by.well.trt 
```

## Notes

It is okay if there are multiple rows in either well quality notes table affect the same well. (This may occur if e.g., all wells on plate X were affected because the plate was shaken and an individual well on plate X had a contamination).  The scripts will collapse these notes and take the minimum wllq for each well.

# Determine if any samples IDs need to be registered

**Background**: In order to process the data with the EPA's ToxCast pipeline and loaded into invitrodb, every treatment must be identified by a sample ID ("spid") registered in ChemTrack. 

Registered sample IDs are usually present in an excel spreadsheet in the project folder (for example, see `L:\Lab\NHEERL_MEA\Project TSCA 2019\EPA_25092_EPA-Shafer_339_20190722_key.xlsx`). The sample ID column is often named "EPA_SAMPLE_ID" or something similar. 

If the sample IDs for a given project have not been registered (whether the test compounds or control compound samples created in the lab for which we want TCPL to make a hit call determination), ask the product owner of ChemTrack to register the samples. For each compound, provide the name of the treatment (as it appears in the MaestroExperimentLog), DTXSID (if available), stock concentration, supplier, lot number, and any additional useful information such as the CASN, purity, etc. The ChemTrack product owner is Madison Feshuk (feshuk.madison@epa.gov) as of Summer 2023.


# Install required R packages

1.	Use the command `install.packages(“package name”)` to install any of the following packages that you do not already have:
a.	`openxlsx` – includes functions for reading .xlsx Excel files
b.	`data.table` – for robust data manipulation
c.	`ggplot2` – plotting package (not essential, recommended)
d.	`stringi` – string matching package (not essential, recommended)
e.	`pracma` – used in mutual information scripts
f.	`compiler` – used in mutual information scripts
g.	`gtools` – includes useful functions such as ‘asc’ for getting ascii character code

2.	Install the package `rhdf5` for reading, writing, and opening h5 files. Use the following commands:

```{r eval = FALSE}
if(!requireNamespace("BiocManager", quietly = TRUE))
  install.packages("BiocManager")
BiocManager::install("rhdf5")
```

If it asks to Update all/some/none packages, select all.
Additional info: https://bioconductor.org/install and  https://stackoverflow.com/questions/15974643/how-to-deal-with-hdf5-files-in-r

3.	Use the command `devtools::install_github("package name")` to install the following packages from GitHub:
a.	`sje30/sjemea`
b.	`dianaransomhall/meadq`

Links to the GitHub repositories: https://github.com/dianaransomhall/meadq, https://github.com/sje30/sjemea

Additional information on installing packages from GitHub:
https://cran.r-project.org/web/packages/githubinstall/vignettes/githubinstall.html

# Running the pre-processing script

## Set up

* Go to the master pre-processing SCRIPTS repository on the L drive (`L:\Lab\NHEERL_MEA\CCTE_Shafer pre-process for TCPL\script_repos\CCTE_Shafer_MEA_dev_pre-processing_scripts\R`) or on GitHub (`https://github.com/amycarpenter/CCTE_Shafer_MEA_dev_pre-processing_scripts_test`). Copy the script `run_me_template.Rmd` and save in the PROJECTS folder you just created (`CCTE_Shafer_MEA_dev_pre-processing_projects\project_name`). Rename the script, replacing `template` with the project name.

* Start RStudio by opening the .Rproj in the PROJECTS folder: `L:\Lab\NHEERL_MEA\CCTE_Shafer pre-process for TCPL\script_repos\CCTE_Shafer_MEA_dev_pre-processing_projects`.

* Open the script you just created (`run_me_[project_name].Rmd`)

* Load the packages in the first chunk. 

* Go the directory containing the pre-processing scripts (e.g., "L:\Lab\NHEERL_MEA\CCTE_Shafer pre-process for TCPL\script_repos\CCTE_Shafer_MEA_dev_pre-processing_scripts\R" or the GitHub repo).

* Fill out the “Project variables definitions” section. Note that all filepaths must use forward slashes ("/") rather than backslashes ("\").
* `project_name` - the project name. Should be the same as the folder name created under 'Setting up the pre-processing project folder'
* `spidmap_file` - path to file that maps treatment names to registered sample IDs. Preferably, this file should also contain the stock concentrations for each sample
* `spid_sheet` - desired sheet in spidmap_file (can be numeric or the sheet name)
* `project.dir` -  main directory of the experimental data for project, e.g., "L:/Lab/NHEERL_MEA/Project - DNT_NTP_2021". This folder is used to direct the scripts of where to search for experimental data files and .txt files that might affect the well quality.
* `scripts.dir` - directory containing the pre-processing scripts for the NFA. Usually "L:\Lab\NHEERL_MEA\CCTE_Shafer pre-process for TCPL\script_repos\CCTE_Shafer_MEA_dev_pre-processing_scripts\R". These scripts are also availabe on GitHub <link tba>
* `root.output.dir` - directory where the project_name folder is located, usually "L:\Lab\NHEERL_MEA\CCTE_Shafer pre-process for TCPL\project_repos\CCTE_Shafer_MEA_dev_pre-processing_projects". This defines where the output will be saved.

* Fill out the information to record the version of the pre-processing scripts that will be used for the given project. 
* To verify that there are no uncommitted modifications to the scripts, open Git Bash in the scripts.dir and run `git status`. Commit any recent modifications and then run `git push` to sync with the remote.
* Enter the branch of the pre-processing scripts that will be used for the given project. Should be the 'master' branch. (Run `git branch` to check which branch is currently displayed in the scripts.dir). 
* Obtain the commit date, message, and SHA of the most recent commit to the scripts repository using either of the following methods
* Navigate to the scripts GitHub repository (https://github.com/amycarpenter/CCTE_Shafer_MEA_dev_pre-processing_scripts_test), click "commits", then view the most recent commit
* Open Git Bash in the scripts.dir. Sync with the remote (run `git pull origin`), then view the most recent commit with `git log -1`
* Enter a link to the GitHub repository for the most recent commit. Should be the GitHub link, followed by "/tree/<SHA>". For example, https://github.com/amycarpenter/CCTE_Shafer_MEA_dev_pre-processing_scripts_test/tree/5c0bf943f9752015ded8e7f54f864b3359e02f10. As the pre-processing scripts are modified going forward, this link should serve as a stable reference to the version of the scripts used to pre-process the current project. Ideally, in the future, the NFA pre-processing scripts could be saved as a package. Then, this step could be replaced by simply referencing the package version.

## Check for READMEs and other well quality notes in project folder 

* The script is set up to scan for any files that end in ".txt" within the project.dir. 
* First, the script will read the contents of the "README" .txt files. These files are created by the lab technicians and may record well quality notes (which may or may not be also be recorded in the lab notebook). Review the contents of each README file and update the well quality tables as needed (see "Adjust well or add well quality notes" section).
* Second, the script will show any additional .txt files found in the project.dir that are not named "README". Check any of open any of these files that look like they may contain an important well quality or other note. Any "Overactive electrodes.txt" files can be ignored.

## Identify source experimental and meta data files

Get a list of the group folders from which you want to pull the experimental data. By default, the script will identify all folders in the project.dir that contain a date (formatted as YYYYMMDD). Modify as needed. Confirm you have the expected number of group folders.

Next we want to get the following files from each group folder:
* 1 Calculations or Summary file containing the cytotoxicity data (LDH and AB) (as well as other data)
* 1 MaestroExperimentLog .csv file which contains the meta data (i.e., the treatment and concentration tested in each well) for each plate tested (usually 3 plates tested per culture/group)
* 1 spike_list.csv file for each of the 4 DIV of recording (DIVs 5, 7, 9, and 12) for each plate

If the group folders are structured as usual, the function 'get_NFA_files_by_group' can be used to get the desired files. However, this function is relatively new and has not been tested on many projects, so it may need to be modified.

Filter the list of all.files as needed. Then, check that the number of files from each group folder matches what you would expect:

* 1 calculations file
* 3 MaestroExperimentLog files (1 for each plate)
* 12 spike list files (4 for each plate)

If the expected number of files are not present in all.files, view the list to determine what is may be missing or added. The function 'basenames()' may be helpful to display just the names of the files (and not the full file paths). Add additional filters or additions as needed.

Finally, the list of files in all.files will be saved in a file name "project_name_files_log.txt".

## Run the main steps

### Convert spike list files to h5 files

This step reads the data in the spike list files, performs some preliminary calculations/summaries, merges in the meta data in the MaestroExperimentLog, and saves all of this information as h5files. One h5file is created for every spike list file. The output will be saved in the project.output.dir.

The primary function is h5_conversion(). This function calls the function axion.spkList.to.h5(), which is in the script `spike_list_functions.R`. 

h5_conversion() arguments:

* project.output.dir - folder in which the h5files should be created
* files.output.dir - folder in which the files_log is located (i.e., the .txt file containing the list of spike list files and MaestroExperimentLog files to be read)
* remake_all - binary value indicating whether new h5files should be created for every spike list file in the files_log (TRUE) or only for the spike list files that have not already been created in the project.output.dir h5files folder (FALSE)
* check_nwells_per_plate - the number of wells that are expected to be present in a plate (usually 48 wells for the NFA). If this many wells are not found in the MaestroExperimentLog, h5_conversion will throw an error
* recording_duration_sec - expected duration of the spike list file recording (in seconds). This should be 900 (i.e., 15 minutes) for Network Formation Assay experiments done in the Shafer lab. Spikes recorded more than recording_duration_sec beyond the time of the first spike will be ignored. If a recording was more than 3 minutes less than the recording_duration_sec, the function will throw an error.

This function will take several minutes to run. As it runs, output will be displayed to the console that summarizes each spike list file. You may see the following message:

> You created a large dataset with compression and chunking.
The chunk size is equal to the dataset dimensions.
If you want to read subsets of the dataset, you should testsmaller chunk sizes to improve read times.

This message indicates that there are more efficient ways to handle the data, but the data itself should not be compromised. So this message can be ignored.

**Notes**: If you have to terminate R part way through this step, the h5file that it was currently working on may be incomplete. Be sure to delete that partial h5file so that it will be recreated when you restart.

**Common issues**:

* If there is no MaestroExperimentLog files that have a culture date and plate.id that match a given spike list file, h5_conversion will throw an error. Check the spike list file and MaestroExperimentLog (both the file name and the contents) for any mismatches in the culture date or plate.id.

### Calculate micro-electrode array parameter values

This step calls the function create_ont_csv(), which is a wrapper for the function create_burst_ont_Data(). The function create_burst_ont_Data() handles the calculation of 16 parameters. (Parameters = mean firing rate, burst rate, number of network spikes, etc). Parameters may also be referred to as "components" or "features". This function takes the h5files as input. The output will be saved in the project.output.dir in a folder called 'prepared_data'. One .csv file will be created for every plate. Within a csv file, every data row corresponds to 1 well from 1 recording (DIV).

**Notes**:

* As the function runs, you will see the following message repeated many times:

> '*** No network spikes found'

This message can be ignored. (Unfortunately, this is produced by a function within either the `meadq` or `sjemea` packages and I do not know how to modify it).

* The output csv files will contain 'ABEfilt' in the file name. This stands for "Actively Bursting Electrodes filter." Briefly, this means that, for relevant parameters, the calculations were restricted to the 'actively bursting electrodes' (i.e., electrodes with >= 0.5 bursts per minute) or the 'active electrodes' (i.e., electrodes with >= 5 spikes per minute) as appropriate for each parameter. See parameter calculation notes or create_burst_ont_Data() for more details.

### Calculate normalized mutual information

This step calculates the normalized mutual information. This is a parameter developed by Ken Ball. This parameter is calculated separately from the other MEA parameters because it was developed separately (by Ken Ball) from the other 16 parameters (developed by Diana Hall and Stephen Eglen) and because it takes significantly longer to calculate than the other parameters. This function takes the h5files as input. The output will be saved in the project.output.dir in a folder called 'All_MI'. One .csv file will be created for every plate. Within a csv file, every data row corresponds to 1 well from 1 recording (DIV).

This is the most computationally-intensive step. Each plate (corresponding to 4 recordings) takes about 10-15 minutes to run.

### Calculate Area-Under-the-Curve values

In order to combine the parameter values from each DIV, we calculate the trapezoidal Area-Under-the-Curve (AUC) values for each parameter value in every well. This step performs that calculation, in addition to some other checks. The function prepare_parameter_values() takes the csv files that contain the 16 parameters and the mutual information for each DIV as input. This function merges all of the files, performs some checks and standardization, merges in the well quality information from the table "[project_name]_well_quality_assignments_by_well.csv" (using the function `add_wllq_by_well.R`), then saves a single .csv with all of the parameter values by DIV. The function parameter_values_to_AUC() read the output file created by prepare_parameter_values(), performs some checks, then calculates the trapzoidal AUC for each parameter in every well over the DIV.

Refer to the scripts prepare_parameter_values.R and parameter_values_to_AUC.R for details on each argument.

**prepare_parameter_values() notes**:

* the default expected_DIVs for this script are c(5, 7, 9, 12)
* NA parameter values are left as NA in output of this function.
* The function prepare_parameter_values() includes an option to add data rows for DIV 2 with a value of 0 for every parameter (see argument add_DIV2_values_of_0). This option should be set to TRUE to be consistent with all Shafer Lab's AUC NFA data pipelined in invitroDB to date (July 2023).
  * Background: Early in the development of the NFA, recordings were made on DIV 2, 5, 7, 9, and 12. Eventually, because so little activity was observed on DIV 2, they stopped recording on DIV 2. When the script `burst_parameter_to_AUC.R` was made (the predecessor to `parameter_values_to_AUC.R`), the writer decided to add a point at (DIV = 2, value = 0) for every well and parameter value. This choice to impute dummy data at DIV 2 has been the practice in the Shafer lab since before 2019. 

* Wllq_by_well notes: 

  * The well quality in the output tables defaults to 1 unless otherwise specified in "[project_name]_well_quality_assignments_by_well.csv". The well quality should not be considered finalized until the well quality notes from the other well quality table are also integrated ("[project_name]_well_quality_assignments_by_compound_cndx_culture_date.csv"). (The second well quality table has to be merged later on in the run_me, after the treatment and concentration is each well is verified).

  * If some of the rows in the wllq.tb.by.well.file that include 'nfa' in the affected assays do not match any of the data rows in the div_data, a warning will be given, e.g.

> Some rows in Test2023_well_quality_assignments_by_well.csv did not match any rows in dat

  This warning indicates that there was likely a typo in the meta data in the wllq.tb.by.well.file or the div_data (e.g., incorrect culture date or Plate.SN entered somewhere). If this warning is observed and the div_data_files includes the entire data set, check the preceding output and check if any of the meta data in the wllq.tb.by.well.file does not match the intended meta data in the div_data.


* Rare-but-sometimes-needed option: interpolate values from non-standard to standard DIVs: Usually, recordings are made on DIV 5, 7, 9, and 12. However, occasionally, e.g., due to inclement weather, recordings may be made on non-standard DIV. For example, if there was inclement weather on DIV 9, a set of plates might have been recorded on DIVs 5, 7, 10, and 12. In order to aim to make the AUC value for these plates comparable to AUC values from other plates with recordings from DIV 5, 7, 9, and 12, the parameter values for DIV 9 can be linearly interpolated from DIVs 7 and 10. This is an imperfect solution, but it has been used occasionally in past projects. 

How values are interpolated with this script:

The function prepare_parameter_values() will check for any non-standard DIV. If any non-standard DIV are present and interpolate_stnd_DIVs = TRUE, then this function will call the function linear_interpolate_DIV() to interpolate the parameter values at the standard DIVs from the DIVs that were recorded. The wllq_notes and file.name in the resulting table will indicate the data rows that were interpolated. However, note that the linear_interpolate_DIV() function was recently updated (July 2023) and has had limited use, so it has not been well-tested and may need to be updated for a given project. 

Note that interpolating values at standard DIVs from non-standard DIVs is not the same as interpolating a value when a DIV recording was just missed. See notes on 'check for missing DIV' below for how this is handled.


**parameter_values_to_AUC() notes**:

* the default expected_DIVs for this script are c(2, 5, 7, 9, 12)
* In order to calculate the area under the curve, this function sets any NA parameter values are set to 0. This has been the practice since before 2019; however, the appropriateness of this choice in uncertain for some parameters. Alternative approaches to summarizing the activity over the DIV recorded may be warranted in these cases.
* **Checks for missing DIVs:** 
  * Case #1: In some cases, a DIV recording may have been missed or not saved for a given plate. For example, a given plate may only have data for DIV 5, 9, and 12 (but not DIV 7). In this case, an AUC value could be calculated, but the appropriateness of comparing this AUC value to AUC values from plates with recordings on all of the expected_DIVs is uncertain. In some past projects (e.g., ~2014) with some missing DIV recordings for a single plate, parameter values from the other 2 plates tested in the same culture were used to generate filler data. Specifically, the median of wells that tested the same treatment and dose was used to generate filler data for each well of the plate with the missing DIV. The script estimate_missing_DIV.R was used to do this. However, this script has not been maintained and this practice is not optimal. Therefore, in the current implementation of `parameter_values_to_AUC()`, the well quality will be set to 0 for the AUC value for any wells/parameters that do not have a value for every DIV in expected_DIVs.
  * Case #2: If the well quality by well is 0 for any DIV for a given well/parameter, than the well quality by well for the AUC value for the well/parameter will also be set to 0.


# RESUME BELOW (then resume with step in run_me, explaining)

Source the run_me script line by line:
* Under the section “run the main steps”, source the script source_steps.R. This script will automatically run through each step. If pause_between_steps is set to TRUE, you will be prompted to enter y/n before continuing each step. The script will also check if a step has been run before. If so, you will be able to select if you want to continue with the existing files, remake all of them (i.e., overwrite), append to the existing files, or quit. You can quit and re-source this line as many times as needed.

## Selecting files
* Select all file types needed for the analysis (_spike_list.csv, _MaestroExperimentLog_Ontogeny.csv, and Calculations/Summary xlsx files containing the cytotoxicity data)
* possibly delineate how the files shoudl be named/formatted?
* When you have selected all files, hit “Cancel” – then the selected files will be saved in a text file.
* possible additiona notes
* only include DIV 5, 7, 9, and 12... but sometimes others
* Don't include DIV 2
* note that if the recording goes past 900 seconds (15 minutes), the spike list file will be truncated at 900 seconds in the script `spike_list_functions.R`.


Potentially useful notes, from the Notes Template:

For the culture dates and plates that we are using, determine where the spike list files are located
1. Determine which culture dates we are using (particularly if some samples/groups were repeated, determine which culture to use (or both))
2. If there are multiple copies/versions of the spike list files in different subfolders, determine which ones you want to use (Some files might go over 900 seconds, some files might have overactive electrodes removed (we want the files without overactive electrodes removed). Note that any data after 900 seconds will be removed in the scripts, so it is okay if the spike list file goes past 900 seconds. 
3. If any recordings are missing, search for them elsewhere on the drive. If you can't find them, ask Kathleen if they are on the DROBO
4. Note if any plates were recorded on any DIV other than 5,7,9,12. We can use a script to interpolate the standard DIV before the AUC calculation
Also find the Master Chem Lists files and the Summary/Calculations files, which contain the Cytotoxicity data (a script will extract the data from these excel/csv files)

* Under the section “prepare spidmap”, you will read in the spidmap_file. (Be sure to close the spidmap file in Excel before reading the file in R). You will need to standardize the names of the treatment, stock_conc, and spid columns in the spidmap. In the line that says “setnames”,
* Update trt_col to the column name in the spidmap that corresponds to the chemical names. The chemical names should match the names in the “treatment” column of the AUC and cytotoxicity data.
* Update the conc_col to the column in the spidmap that lists the stock concentration of the chemicals (this will be used to confirm the concentration-correction where the stock concentration is not exactly 20mM).
* Update the spid_col to the column in the spidmap that lists the EPA Sample ID corresponding to each compound. This column is . The sample IDs (or SPIDs) usually begin with a prefix such as “EPA”,“EX”, “TP” or “TX” followed by a 6-8 digit code.
* If you need multiple spidmaps, you can read them in separately and then combine them with rbind.
* The “expected_stock_conc” is the target concentration. This is the expected concentration that the dilutions were based on. This is usually 20mM. Sometimes, it is 30mM. Sometimes, for individual compounds in a dataset, the lab technician sees that the actual concentration is not 20, and so adjust the source_conc’s accordingly… more to explain
* Show examples, esp where expected is 10 and actual is 10.1?

*	Run tcpl_MEA_dev_AUC
* If you get an error stating that some treatments don’t have a corresponding spid in the spidmap, you may need to rename any compounds that were misspelled in the auc/cytotox data. Uncomment the section under “rename any compounds” and update as needed.
* This script will also check the concentration corrections for each compound. Follow the prompts to update any concentrations that look off. It assumes that the expected aliquot concentration for each compound is 20. If that is not the case, you can change this default by adding the argument expected_target_conc = 30 (for example)
* Other things to check:
* If it appears that the conc’s were partially conc-corrected (e.g. corrected in cyto data but not AUC dat) -> need to standardize the conc’s before you can continue	
*	Suggest how user could do that??
* 	If it appears that the conc’s were conc-corrected incorrectly (show example -> spidmap_guess_conc’s does not agree with actual conc’s, but actual conc’s are not 0.1,0.3,etc) -> Then need to standardized the concs or something before can correct them

*	Run the final data checks
* Read through the output and confirm that there are the expected number of cultures, plates, etc., no missing data, etc.
* Take a look at the output plots. There isn’t really anything specific to look for in the plots… just check that nothing looks waay off, many missing values, etc. Compare control wells to treated values in each plot and see if it looks reasonable.
* Feel free to do any other checks that you want!
In the end, you should have a file in the output folder called “datasetname_longfile.csv”
Once you have successfully made it through all of the steps, set save_notes_graphs to TRUE and pause_between_steps to FALSE. Then source the entire “run_me” script again. A “run_log” text file and a “summary_plots” folder will be created as documentation.

Conc-correction function:
*	If a compound in your dataset is tested at different concentrations than those listed under “expected_target_concs”, this compound will be flagged as a compound whose concentrations should be corrected. However, if the stock concentration is 20, the concentration-correction will not affect these values.
* Not a great long-term solution, because what if the stock conc is not 20!

Dataset checks
-	Make sure you review the results, make sure the values look reasonable, no glaring unexpected holes!

Once you get to the end, re-run from the beginning with these settings in the `USER INPUT` section: 
```{r}
pause_between_steps <- TRUE
save_notes_graphs <- FALSE
```

This will save the graphs and dataset checks in pdf and txt files.

# Wrap up

Knit the doc to save it!

# Final checks before sending data to TCPL

(per recommendation from Kelly)

a)	Have the lab look at the data, particularly to make sure that the chemicals that were expected to be present are all there (nothing missing, nothing added, probably expected number of replicates, conc range, etc).
b)	Have a biologist look at the data, see if things look as expected


